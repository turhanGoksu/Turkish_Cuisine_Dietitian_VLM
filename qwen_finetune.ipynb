{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q -U torch torchvision transformers peft bitsandbytes accelerate datasets trl qwen-vl-utils\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "print(f\"âœ… Setup complete. Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {
        "id": "snytyctyMhso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "# Paths (Update these paths for your environment)\n",
        "DATASET_PATH = \"/content/drive/MyDrive/yemek/yemek_dataset_v3_corrected.csv\"\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/yemek/\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/outputs_v3_expert\"\n",
        "\n",
        "# Training Hyperparameters\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 8\n",
        "GRAD_ACCUMULATION = 2\n",
        "LORA_RANK = 64\n",
        "LORA_ALPHA = 128"
      ],
      "metadata": {
        "id": "IMHdlAUzMi9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Config (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Base Model\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load Processor\n",
        "processor = Qwen2VLProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "ickYP3I0MlCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# System Prompt for the AI Dietitian role\n",
        "SYSTEM_PROMPT = \"Sen TÃ¼rk mutfaÄŸÄ± konusunda uzman bir diyetisyen ve VLM asistanÄ±sÄ±n. Yemekleri tanÄ± ve besin deÄŸerlerini sÃ¶yle.\"\n",
        "\n",
        "def format_data(sample):\n",
        "    \"\"\"Formats the dataset into Qwen2-VL conversation structure.\"\"\"\n",
        "    try:\n",
        "        image_path = os.path.join(IMAGE_DIR, sample[\"image_path\"])\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": sample[\"question\"]}]},\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"answer\"]}]}\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Load and Split Data\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "train_df, eval_df = train_test_split(df, test_size=0.05, random_state=42)\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = [res for x in train_df.to_dict('records') if (res := format_data(x))]\n",
        "eval_dataset = [res for x in eval_df.to_dict('records') if (res := format_data(x))]\n",
        "\n",
        "print(f\"Train Size: {len(train_dataset)} | Eval Size: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "D4Lg-xbgMmgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=0.05,\n",
        "    r=LORA_RANK,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Data Collator\n",
        "def collate_fn(examples):\n",
        "    texts = [processor.apply_chat_template(ex, tokenize=False, add_generation_prompt=False) for ex in examples]\n",
        "    images = [ex[1][\"content\"][0][\"image\"] for ex in examples]\n",
        "\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "    return batch\n",
        "\n",
        "# Training Arguments\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    save_steps=500,\n",
        "    logging_steps=10,\n",
        "    bf16=True,\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "y4El_JalMoHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    processing_class=processor.tokenizer,\n",
        ")\n",
        "\n",
        "# Start Training\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save Model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "print(f\"âœ… Model saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "4aIkLzReMpi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_inference(image_path, question=\"Bu yemek nedir?\"):\n",
        "    \"\"\"Runs inference on a single image.\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": question}]}\n",
        "    ]\n",
        "\n",
        "    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "    inputs = processor(text=[text_prompt], images=[image], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "    response = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Display\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    print(f\"Answer: {response.split('assistant')[-1].strip()}\")\n",
        "\n",
        "# Example Usage (Update path)\n",
        "# run_inference(\"/content/drive/MyDrive/test_images/kebab.jpg\")"
      ],
      "metadata": {
        "id": "aGOGPMvzMrGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}